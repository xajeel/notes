# **Training a CNN on a Custom Dataset**  

---

### **Lesson Objectives**  
By the end of this lesson, students will learn:  
âœ… How to load a **custom image dataset** in TensorFlow  
âœ… How to apply **data augmentation** to improve performance  
âœ… How to **split the dataset** into training, validation, and test sets  
âœ… How to build a **CNN model** for the custom dataset  
âœ… How to **train the model** and evaluate its performance  

---

## **Lesson Outline**  

### **1ï¸âƒ£ Loading a Custom Dataset**  
- Introduction to image datasets  
- Using `ImageDataGenerator` and `tf.keras.utils.image_dataset_from_directory`  

### **2ï¸âƒ£ Data Augmentation**  
- Why data augmentation is important  
- Different augmentation techniques  

### **3ï¸âƒ£ Splitting the Dataset**  
- Why we need **training, validation, and test sets**  
- How to split data using TensorFlow  

### **4ï¸âƒ£ Building a CNN Model**  
- Creating a CNN using `Conv2D`, `MaxPooling2D`, and `Dense` layers  

### **5ï¸âƒ£ Training and Evaluating the Model**  
- How to train the CNN  
- How to check accuracy and loss  
- Making predictions  

---

## **1ï¸âƒ£ Loading a Custom Dataset**  

### **Why Do We Need a Custom Dataset?**  
MNIST contains handwritten digits, but real-world problems involve different kinds of images. Suppose we want to build a model to classify **dogs vs. cats**â€”we need to load a dataset containing **dog** and **cat** images.  

ğŸ¨ **Analogy:** Think of a CNN as a **student learning from examples**. If we want it to recognize dogs and cats, we must **provide images** of both!  

---

### **ğŸ“Œ Loading the Dataset**  
We'll assume we have a dataset with two folders:  
ğŸ“‚ `dataset/`  
- ğŸ“ `train/`  
  - ğŸ“ `dogs/` ğŸ¶  
  - ğŸ“ `cats/` ğŸ±  
- ğŸ“ `test/`  
  - ğŸ“ `dogs/`  
  - ğŸ“ `cats/`  

**TensorFlow Code to Load Images:**  

```python
import tensorflow as tf

data_dir = '/content/1/training_set/training_set'

# Class names (dogs, cats)
class_names = train_data.class_names
print("Class labels:", class_names)

```

```python

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(150, 150),
  batch_size=32)

val_ds  = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(150, 150),
  batch_size=32)

class_names = train_ds.class_names
print(class_names)
```

---

## **2ï¸âƒ£ Data Augmentation**  

### **What is Data Augmentation?**  
ğŸ“Œ Data augmentation **creates new variations** of images to improve model learning.  

ğŸ¨ **Analogy:** If a student only sees one style of handwriting, they might struggle with new handwriting styles. Data augmentation **teaches the CNN different styles!**  

**Common Augmentation Techniques:**  
âœ… **Flipping** â€“ Mirror images horizontally  
âœ… **Rotation** â€“ Rotate images slightly  
âœ… **Zooming** â€“ Zoom in on objects  
âœ… **Brightness Change** â€“ Simulate different lighting  

**TensorFlow Code for Data Augmentation:**  

```python
from tensorflow.keras import layers

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),  # Flip images
    layers.RandomRotation(0.2),       # Rotate images
    layers.RandomZoom(0.2),           # Zoom in images
    layers.RandomContrast(0.2)        # Adjust contrast
])
```

Normalizing the Images

```python
normalization_layer = tf.keras.layers.Rescaling(1./255)
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
```

---

## **4ï¸âƒ£ Building a CNN Model**  

**ğŸ“Œ CNN Architecture for Image Classification:**  

```python
from tensorflow.keras import layers, models

# Create CNN model


model = tf.keras.Sequential([
  tf.keras.layers.Rescaling(1./255),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(2)
])

# Compile the model
model.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

# Show model summary
model.summary()
```

---

## **5ï¸âƒ£ Training and Evaluating the Model**  

### **ğŸ“Œ Training the Model:**  

```python
model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=3
)
```

### **ğŸ“Œ Evaluating on Test Data:**  

```python
test_loss, test_acc = model.evaluate(test_data)
print(f"Test accuracy: {test_acc:.4f}")
```

---

## **Conclusion ğŸ¯**  
âœ” We **loaded** a custom dataset of images.  
âœ” We **augmented** the data for better generalization.  
âœ” We **split** the dataset into training, validation, and test sets.  
âœ” We **built and trained** a CNN for image classification.  
âœ” We **evaluated and tested** the model's performance.  

---